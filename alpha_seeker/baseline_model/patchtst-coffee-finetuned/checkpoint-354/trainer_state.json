{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 354,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0423728813559322,
      "grad_norm": 112.65262603759766,
      "learning_rate": 0.0004,
      "loss": 83.6791,
      "step": 5
    },
    {
      "epoch": 0.0847457627118644,
      "grad_norm": 19.495195388793945,
      "learning_rate": 0.0009000000000000001,
      "loss": 87.8676,
      "step": 10
    },
    {
      "epoch": 0.1271186440677966,
      "grad_norm": 65.08940887451172,
      "learning_rate": 0.0009883720930232557,
      "loss": 69.291,
      "step": 15
    },
    {
      "epoch": 0.1694915254237288,
      "grad_norm": 399.2413024902344,
      "learning_rate": 0.0009738372093023255,
      "loss": 50.0448,
      "step": 20
    },
    {
      "epoch": 0.211864406779661,
      "grad_norm": 835.4649658203125,
      "learning_rate": 0.0009593023255813954,
      "loss": 56.5484,
      "step": 25
    },
    {
      "epoch": 0.2542372881355932,
      "grad_norm": 416.83929443359375,
      "learning_rate": 0.0009447674418604651,
      "loss": 93.391,
      "step": 30
    },
    {
      "epoch": 0.2966101694915254,
      "grad_norm": 276.43218994140625,
      "learning_rate": 0.0009302325581395349,
      "loss": 95.1882,
      "step": 35
    },
    {
      "epoch": 0.3389830508474576,
      "grad_norm": 357.0552062988281,
      "learning_rate": 0.0009156976744186047,
      "loss": 90.473,
      "step": 40
    },
    {
      "epoch": 0.3813559322033898,
      "grad_norm": 205.31820678710938,
      "learning_rate": 0.0009011627906976745,
      "loss": 34.3467,
      "step": 45
    },
    {
      "epoch": 0.423728813559322,
      "grad_norm": 327.8268737792969,
      "learning_rate": 0.0008866279069767442,
      "loss": 35.9965,
      "step": 50
    },
    {
      "epoch": 0.4661016949152542,
      "grad_norm": 750.0146484375,
      "learning_rate": 0.0008720930232558139,
      "loss": 40.7511,
      "step": 55
    },
    {
      "epoch": 0.5084745762711864,
      "grad_norm": 520.0097045898438,
      "learning_rate": 0.0008575581395348837,
      "loss": 69.0228,
      "step": 60
    },
    {
      "epoch": 0.5508474576271186,
      "grad_norm": 971.2850952148438,
      "learning_rate": 0.0008430232558139536,
      "loss": 47.0842,
      "step": 65
    },
    {
      "epoch": 0.5932203389830508,
      "grad_norm": 304.6527099609375,
      "learning_rate": 0.0008284883720930233,
      "loss": 56.0204,
      "step": 70
    },
    {
      "epoch": 0.635593220338983,
      "grad_norm": 208.75338745117188,
      "learning_rate": 0.0008139534883720931,
      "loss": 21.2655,
      "step": 75
    },
    {
      "epoch": 0.6779661016949152,
      "grad_norm": 231.40341186523438,
      "learning_rate": 0.0007994186046511628,
      "loss": 31.6305,
      "step": 80
    },
    {
      "epoch": 0.7203389830508474,
      "grad_norm": 263.0784912109375,
      "learning_rate": 0.0007848837209302325,
      "loss": 36.6582,
      "step": 85
    },
    {
      "epoch": 0.7627118644067796,
      "grad_norm": 939.3438720703125,
      "learning_rate": 0.0007703488372093023,
      "loss": 34.7243,
      "step": 90
    },
    {
      "epoch": 0.8050847457627118,
      "grad_norm": 938.381103515625,
      "learning_rate": 0.0007558139534883722,
      "loss": 38.6312,
      "step": 95
    },
    {
      "epoch": 0.847457627118644,
      "grad_norm": 56.55886459350586,
      "learning_rate": 0.0007412790697674419,
      "loss": 27.0221,
      "step": 100
    },
    {
      "epoch": 0.8898305084745762,
      "grad_norm": 495.1221008300781,
      "learning_rate": 0.0007267441860465116,
      "loss": 34.3664,
      "step": 105
    },
    {
      "epoch": 0.9322033898305084,
      "grad_norm": 253.5347442626953,
      "learning_rate": 0.0007122093023255815,
      "loss": 42.7225,
      "step": 110
    },
    {
      "epoch": 0.9745762711864406,
      "grad_norm": 357.4148864746094,
      "learning_rate": 0.0006976744186046512,
      "loss": 33.4634,
      "step": 115
    },
    {
      "epoch": 1.0,
      "eval_runtime": 0.0685,
      "eval_samples_per_second": 3444.606,
      "eval_steps_per_second": 437.874,
      "step": 118
    },
    {
      "epoch": 1.0169491525423728,
      "grad_norm": 685.6817016601562,
      "learning_rate": 0.0006831395348837209,
      "loss": 47.4267,
      "step": 120
    },
    {
      "epoch": 1.0593220338983051,
      "grad_norm": 567.374755859375,
      "learning_rate": 0.0006686046511627907,
      "loss": 34.8966,
      "step": 125
    },
    {
      "epoch": 1.1016949152542372,
      "grad_norm": 540.9810180664062,
      "learning_rate": 0.0006540697674418606,
      "loss": 38.8851,
      "step": 130
    },
    {
      "epoch": 1.1440677966101696,
      "grad_norm": 370.9481201171875,
      "learning_rate": 0.0006395348837209303,
      "loss": 15.5449,
      "step": 135
    },
    {
      "epoch": 1.1864406779661016,
      "grad_norm": 423.10528564453125,
      "learning_rate": 0.000625,
      "loss": 21.5196,
      "step": 140
    },
    {
      "epoch": 1.228813559322034,
      "grad_norm": 786.8659057617188,
      "learning_rate": 0.0006104651162790697,
      "loss": 38.1268,
      "step": 145
    },
    {
      "epoch": 1.271186440677966,
      "grad_norm": 662.494384765625,
      "learning_rate": 0.0005959302325581395,
      "loss": 28.2636,
      "step": 150
    },
    {
      "epoch": 1.3135593220338984,
      "grad_norm": 204.0193328857422,
      "learning_rate": 0.0005813953488372093,
      "loss": 43.8415,
      "step": 155
    },
    {
      "epoch": 1.3559322033898304,
      "grad_norm": 202.5369873046875,
      "learning_rate": 0.0005668604651162791,
      "loss": 28.6328,
      "step": 160
    },
    {
      "epoch": 1.3983050847457628,
      "grad_norm": 298.5932922363281,
      "learning_rate": 0.0005523255813953489,
      "loss": 40.9926,
      "step": 165
    },
    {
      "epoch": 1.4406779661016949,
      "grad_norm": 553.0502319335938,
      "learning_rate": 0.0005377906976744186,
      "loss": 50.5715,
      "step": 170
    },
    {
      "epoch": 1.4830508474576272,
      "grad_norm": 731.859619140625,
      "learning_rate": 0.0005232558139534884,
      "loss": 38.8804,
      "step": 175
    },
    {
      "epoch": 1.5254237288135593,
      "grad_norm": 139.0940399169922,
      "learning_rate": 0.0005087209302325581,
      "loss": 37.8047,
      "step": 180
    },
    {
      "epoch": 1.5677966101694916,
      "grad_norm": 513.4471435546875,
      "learning_rate": 0.0004941860465116279,
      "loss": 31.4371,
      "step": 185
    },
    {
      "epoch": 1.6101694915254239,
      "grad_norm": 288.4189758300781,
      "learning_rate": 0.0004796511627906977,
      "loss": 42.2985,
      "step": 190
    },
    {
      "epoch": 1.652542372881356,
      "grad_norm": 300.99688720703125,
      "learning_rate": 0.00046511627906976747,
      "loss": 52.3815,
      "step": 195
    },
    {
      "epoch": 1.694915254237288,
      "grad_norm": 608.5354614257812,
      "learning_rate": 0.00045058139534883725,
      "loss": 31.821,
      "step": 200
    },
    {
      "epoch": 1.7372881355932204,
      "grad_norm": 315.88238525390625,
      "learning_rate": 0.00043604651162790697,
      "loss": 30.0389,
      "step": 205
    },
    {
      "epoch": 1.7796610169491527,
      "grad_norm": 582.21533203125,
      "learning_rate": 0.0004215116279069768,
      "loss": 44.5687,
      "step": 210
    },
    {
      "epoch": 1.8220338983050848,
      "grad_norm": 78.5267333984375,
      "learning_rate": 0.00040697674418604653,
      "loss": 32.8889,
      "step": 215
    },
    {
      "epoch": 1.8644067796610169,
      "grad_norm": 781.990478515625,
      "learning_rate": 0.00039244186046511625,
      "loss": 37.4357,
      "step": 220
    },
    {
      "epoch": 1.9067796610169492,
      "grad_norm": 157.0337677001953,
      "learning_rate": 0.0003779069767441861,
      "loss": 55.7878,
      "step": 225
    },
    {
      "epoch": 1.9491525423728815,
      "grad_norm": 508.1011657714844,
      "learning_rate": 0.0003633720930232558,
      "loss": 45.6337,
      "step": 230
    },
    {
      "epoch": 1.9915254237288136,
      "grad_norm": 613.2447509765625,
      "learning_rate": 0.0003488372093023256,
      "loss": 42.9456,
      "step": 235
    },
    {
      "epoch": 2.0,
      "eval_runtime": 0.1056,
      "eval_samples_per_second": 2234.841,
      "eval_steps_per_second": 284.09,
      "step": 236
    },
    {
      "epoch": 2.0338983050847457,
      "grad_norm": 215.29554748535156,
      "learning_rate": 0.00033430232558139537,
      "loss": 19.6417,
      "step": 240
    },
    {
      "epoch": 2.0762711864406778,
      "grad_norm": 303.7762145996094,
      "learning_rate": 0.00031976744186046514,
      "loss": 30.1479,
      "step": 245
    },
    {
      "epoch": 2.1186440677966103,
      "grad_norm": 131.0049285888672,
      "learning_rate": 0.00030523255813953487,
      "loss": 30.4715,
      "step": 250
    },
    {
      "epoch": 2.1610169491525424,
      "grad_norm": 414.0100402832031,
      "learning_rate": 0.00029069767441860465,
      "loss": 41.2701,
      "step": 255
    },
    {
      "epoch": 2.2033898305084745,
      "grad_norm": 423.36865234375,
      "learning_rate": 0.0002761627906976744,
      "loss": 38.1491,
      "step": 260
    },
    {
      "epoch": 2.2457627118644066,
      "grad_norm": 393.3999328613281,
      "learning_rate": 0.0002616279069767442,
      "loss": 32.1406,
      "step": 265
    },
    {
      "epoch": 2.288135593220339,
      "grad_norm": 135.4523162841797,
      "learning_rate": 0.00024709302325581393,
      "loss": 31.2492,
      "step": 270
    },
    {
      "epoch": 2.330508474576271,
      "grad_norm": 350.2176818847656,
      "learning_rate": 0.00023255813953488373,
      "loss": 54.1995,
      "step": 275
    },
    {
      "epoch": 2.3728813559322033,
      "grad_norm": 150.65174865722656,
      "learning_rate": 0.00021802325581395349,
      "loss": 32.487,
      "step": 280
    },
    {
      "epoch": 2.415254237288136,
      "grad_norm": 287.9229431152344,
      "learning_rate": 0.00020348837209302326,
      "loss": 36.8573,
      "step": 285
    },
    {
      "epoch": 2.457627118644068,
      "grad_norm": 497.5414733886719,
      "learning_rate": 0.00018895348837209304,
      "loss": 24.9111,
      "step": 290
    },
    {
      "epoch": 2.5,
      "grad_norm": 462.22882080078125,
      "learning_rate": 0.0001744186046511628,
      "loss": 34.3402,
      "step": 295
    },
    {
      "epoch": 2.542372881355932,
      "grad_norm": 133.74990844726562,
      "learning_rate": 0.00015988372093023257,
      "loss": 29.2616,
      "step": 300
    },
    {
      "epoch": 2.584745762711864,
      "grad_norm": 323.1987609863281,
      "learning_rate": 0.00014534883720930232,
      "loss": 32.9696,
      "step": 305
    },
    {
      "epoch": 2.6271186440677967,
      "grad_norm": 805.1104736328125,
      "learning_rate": 0.0001308139534883721,
      "loss": 24.4955,
      "step": 310
    },
    {
      "epoch": 2.669491525423729,
      "grad_norm": 1010.0560913085938,
      "learning_rate": 0.00011627906976744187,
      "loss": 45.7888,
      "step": 315
    },
    {
      "epoch": 2.711864406779661,
      "grad_norm": 348.4879150390625,
      "learning_rate": 0.00010174418604651163,
      "loss": 48.8728,
      "step": 320
    },
    {
      "epoch": 2.7542372881355934,
      "grad_norm": 366.8946228027344,
      "learning_rate": 8.72093023255814e-05,
      "loss": 30.9603,
      "step": 325
    },
    {
      "epoch": 2.7966101694915255,
      "grad_norm": 129.88829040527344,
      "learning_rate": 7.267441860465116e-05,
      "loss": 24.6597,
      "step": 330
    },
    {
      "epoch": 2.8389830508474576,
      "grad_norm": 636.3736572265625,
      "learning_rate": 5.8139534883720933e-05,
      "loss": 51.4781,
      "step": 335
    },
    {
      "epoch": 2.8813559322033897,
      "grad_norm": 204.76564025878906,
      "learning_rate": 4.36046511627907e-05,
      "loss": 23.1286,
      "step": 340
    },
    {
      "epoch": 2.923728813559322,
      "grad_norm": 80.01670837402344,
      "learning_rate": 2.9069767441860467e-05,
      "loss": 30.3166,
      "step": 345
    },
    {
      "epoch": 2.9661016949152543,
      "grad_norm": 122.26847076416016,
      "learning_rate": 1.4534883720930233e-05,
      "loss": 35.7509,
      "step": 350
    },
    {
      "epoch": 3.0,
      "eval_runtime": 0.0682,
      "eval_samples_per_second": 3459.752,
      "eval_steps_per_second": 439.799,
      "step": 354
    }
  ],
  "logging_steps": 5,
  "max_steps": 354,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 25767779400.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
